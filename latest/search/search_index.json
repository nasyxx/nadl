{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>nadl</code>","text":"<p>Nasy's JAX/FLAX deep learning toolkit.</p>"},{"location":"api/","title":"API","text":"<p>Python \u2661 Nasy.</p> <pre><code>|             *         *\n|                  .                .\n|           .                              \u767b\n|     *                      ,\n|                   .                      \u81f3\n|\n|                               *          \u6056\n|          |\\___/|\n|          )    -(             .           \u8056 \u00b7\n|         =\\ -   /=\n|           )===(       *\n|          /   - \\\n|          |-    |\n|         /   -   \\     0.|.0\n|  NASY___\\__( (__/_____(\\=/)__+1s____________\n|  ______|____) )______|______|______|______|_\n|  ___|______( (____|______|______|______|____\n|  ______|____\\_|______|______|______|______|_\n|  ___|______|______|______|______|______|____\n|  ______|______|______|______|______|______|_\n|  ___|______|______|______|______|______|____\n</code></pre> <p>author   : Nasy https://nasy.moe date     : Nov 29, 2023 email    : Nasy nasyxx+python@gmail.com filename : init.py project  : nadl license  : GPL-3.0+</p> <p>NADL</p>"},{"location":"api/#nadl.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>Metric</code></p> <p>Accuracy.</p> Source code in <code>nadl/metrics.py</code> <pre><code>class Accuracy(Metric):\n  \"\"\"Accuracy.\"\"\"\n\n  labels: Int[Array, \"...\"]\n  preds: Int[Array, \"...\"]\n  name: str = \"accuracy\"\n\n  def compute(self) -&gt; Array:\n    \"\"\"Compute.\"\"\"\n    return jnp.nanmean(self.labels == self.preds, axis=-1)\n</code></pre>"},{"location":"api/#nadl.Accuracy.compute","title":"<code>compute()</code>","text":"<p>Compute.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def compute(self) -&gt; Array:\n  \"\"\"Compute.\"\"\"\n  return jnp.nanmean(self.labels == self.preds, axis=-1)\n</code></pre>"},{"location":"api/#nadl.BaseTrainState","title":"<code>BaseTrainState</code>","text":"<p>               Bases: <code>Module</code></p> <p>Train state.</p> Source code in <code>nadl/states.py</code> <pre><code>class BaseTrainState[T, M](eqx.Module):\n  \"\"\"Train state.\"\"\"\n\n  model: M\n  tx: optax.GradientTransformation\n  opt_state: optax.OptState\n  loss: jax.Array\n  step: jax.Array\n  conf: T\n\n  @classmethod\n  @abstractmethod\n  def init[**P](cls: type[Self], *args: P.args, **kwds: P.kwargs) -&gt; Self:\n    \"\"\"Create initial state.\"\"\"\n    raise NotImplementedError\n\n  def apply_grads(self, loss: jax.Array, grads: eqx.Module) -&gt; BaseTrainState[T, M]:\n    \"\"\"Apply gradients.\"\"\"\n    updates, opt_state = self.tx.update(\n      cast(optax.Updates, grads), self.opt_state, params=cast(optax.Params, self.model)\n    )\n    model = eqx.apply_updates(self.model, updates)\n    return eqx.tree_at(\n      lambda x: (x.model, x.opt_state, x.loss, x.step),\n      self,\n      (model, opt_state, loss, self.step + 1),\n    )\n</code></pre>"},{"location":"api/#nadl.BaseTrainState.apply_grads","title":"<code>apply_grads(loss, grads)</code>","text":"<p>Apply gradients.</p> Source code in <code>nadl/states.py</code> <pre><code>def apply_grads(self, loss: jax.Array, grads: eqx.Module) -&gt; BaseTrainState[T, M]:\n  \"\"\"Apply gradients.\"\"\"\n  updates, opt_state = self.tx.update(\n    cast(optax.Updates, grads), self.opt_state, params=cast(optax.Params, self.model)\n  )\n  model = eqx.apply_updates(self.model, updates)\n  return eqx.tree_at(\n    lambda x: (x.model, x.opt_state, x.loss, x.step),\n    self,\n    (model, opt_state, loss, self.step + 1),\n  )\n</code></pre>"},{"location":"api/#nadl.BaseTrainState.init","title":"<code>init(*args, **kwds)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create initial state.</p> Source code in <code>nadl/states.py</code> <pre><code>@classmethod\n@abstractmethod\ndef init[**P](cls: type[Self], *args: P.args, **kwds: P.kwargs) -&gt; Self:\n  \"\"\"Create initial state.\"\"\"\n  raise NotImplementedError\n</code></pre>"},{"location":"api/#nadl.DState","title":"<code>DState</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Dataloader state.</p> Source code in <code>nadl/data.py</code> <pre><code>class DState[T](NamedTuple):\n  \"\"\"Dataloader state.\"\"\"\n\n  xs: T\n  pad: Bool[Array, \" b\"]\n  length: Int[Array, \"\"]\n  epoch: Int[Array, \"\"] = jnp.asarray(0)\n  step: Int[Array, \"\"] = jnp.asarray(0)\n  key: PRNGKeyArray = field(default_factory=lambda: jax.random.key(42))\n  name: str | None = None\n</code></pre>"},{"location":"api/#nadl.DataLoader","title":"<code>DataLoader</code>","text":"<p>               Bases: <code>Module</code></p> <p>Simple data loader.</p> Source code in <code>nadl/data.py</code> <pre><code>class DataLoader[T](Module):\n  \"\"\"Simple data loader.\"\"\"\n\n  gen: _IDX_FN\n  embed: TransT[Int[Array, \" d\"], T]\n  transform: TransT[T, T]\n  _default_epoch: Int[Array, \"\"]\n  _length: int\n  _data_length: int\n  _batch_size: int\n\n  def __init__(\n    self,\n    length: int,\n    batch_size: int,\n    drop_last: bool = False,\n    shuffle: bool = False,\n    key: PRNGKeyArray | None = None,\n    *,\n    embed: TransT[Int[Array, \" d\"], T] | None = None,\n    transform: TransT[T, T] | None = None,\n  ) -&gt; None:\n    \"\"\"Initiate the dataloader.\"\"\"\n    self.gen = batch_index(length, batch_size, drop_last, shuffle, key=key)\n    self.embed = (\n      embed if embed is not None else cast(TransT[Int[Array, \" d\"], T], Identity())\n    )\n    self.transform = (\n      transform if transform is not None else cast(TransT[T, T], Identity())\n    )\n    # why jit make it slower?\n    # self.embed = filter_jit(self.embed)\n    # self.transform = filter_jit(self.transform)\n    self._default_epoch = jnp.asarray(0)\n    self._length = self.gen(self._default_epoch).length.item()\n    self._data_length = length\n    self._batch_size = batch_size\n\n  def __call__(self, epoch: Int[Array, \"\"] | None = None) -&gt; Iterator[DState[T]]:\n    \"\"\"Get the indexes.\"\"\"\n    data = (\n      filter_jit(self.gen)(self._default_epoch) if epoch is None else self.gen(epoch)\n    )\n    for step, d, p, k in zip(jnp.arange(len(self)) + 1, data.xs, data.pad, data.key):\n      yield DState(\n        self.transform(self.embed(d, key=k), key=k),\n        p,\n        data.length,\n        data.epoch,\n        step,\n        k,\n      )\n\n  @filter_jit\n  def _ex(self, x: Array) -&gt; Array:\n    \"\"\"Ex epoch and steps.\n\n    epoch len *batch -&gt; (epoch len) * batch\n    \"\"\"\n    if x.ndim &gt; 1:\n      return x.reshape(-1, *x.shape[2:])\n    return repeat(x, \"e ... -&gt; (e l) ...\", l=self._length)\n\n  def epoch_iter(\n    self, epoch_start: int, epoch_end: int | None = None\n  ) -&gt; Iterator[DState[T]]:\n    \"\"\"Iterate with epochs.\"\"\"\n    gen = filter_jit(filter_vmap(self.gen))\n    # steps = jnp.arange(self._length)\n    if epoch_end is None:\n      epoch_end = epoch_start + 1\n      epoch_start = 1\n    data = gen(jnp.arange(epoch_start, epoch_end))\n\n    data = jax.tree.map(self._ex, data)\n    for step, x, pad, length, epoch, key in zip(\n      jnp.arange(data.xs.shape[0]) + 1,\n      data.xs,\n      data.pad,\n      data.length,\n      data.epoch,\n      data.key,\n    ):\n      yield DState(\n        self.transform(self.embed(x, key=key), key=key),\n        pad,\n        length,\n        epoch,\n        step,\n        key,\n      )\n\n  def viter(\n    self, epochs: int, chunks: int = 100, *, epoch_bias: int = 1\n  ) -&gt; Iterator[DState[T]]:\n    \"\"\"Iterate with epochs and chunks.\"\"\"\n    for chunk in range(epoch_bias, epochs + epoch_bias, chunks):\n      yield from self.epoch_iter(chunk, min(chunk + chunks, epochs + epoch_bias))\n\n  def __len__(self) -&gt; int:\n    \"\"\"Get the length.\"\"\"\n    return self._length\n</code></pre>"},{"location":"api/#nadl.DataLoader.__call__","title":"<code>__call__(epoch=None)</code>","text":"<p>Get the indexes.</p> Source code in <code>nadl/data.py</code> <pre><code>def __call__(self, epoch: Int[Array, \"\"] | None = None) -&gt; Iterator[DState[T]]:\n  \"\"\"Get the indexes.\"\"\"\n  data = (\n    filter_jit(self.gen)(self._default_epoch) if epoch is None else self.gen(epoch)\n  )\n  for step, d, p, k in zip(jnp.arange(len(self)) + 1, data.xs, data.pad, data.key):\n    yield DState(\n      self.transform(self.embed(d, key=k), key=k),\n      p,\n      data.length,\n      data.epoch,\n      step,\n      k,\n    )\n</code></pre>"},{"location":"api/#nadl.DataLoader.__init__","title":"<code>__init__(length, batch_size, drop_last=False, shuffle=False, key=None, *, embed=None, transform=None)</code>","text":"<p>Initiate the dataloader.</p> Source code in <code>nadl/data.py</code> <pre><code>def __init__(\n  self,\n  length: int,\n  batch_size: int,\n  drop_last: bool = False,\n  shuffle: bool = False,\n  key: PRNGKeyArray | None = None,\n  *,\n  embed: TransT[Int[Array, \" d\"], T] | None = None,\n  transform: TransT[T, T] | None = None,\n) -&gt; None:\n  \"\"\"Initiate the dataloader.\"\"\"\n  self.gen = batch_index(length, batch_size, drop_last, shuffle, key=key)\n  self.embed = (\n    embed if embed is not None else cast(TransT[Int[Array, \" d\"], T], Identity())\n  )\n  self.transform = (\n    transform if transform is not None else cast(TransT[T, T], Identity())\n  )\n  # why jit make it slower?\n  # self.embed = filter_jit(self.embed)\n  # self.transform = filter_jit(self.transform)\n  self._default_epoch = jnp.asarray(0)\n  self._length = self.gen(self._default_epoch).length.item()\n  self._data_length = length\n  self._batch_size = batch_size\n</code></pre>"},{"location":"api/#nadl.DataLoader.__len__","title":"<code>__len__()</code>","text":"<p>Get the length.</p> Source code in <code>nadl/data.py</code> <pre><code>def __len__(self) -&gt; int:\n  \"\"\"Get the length.\"\"\"\n  return self._length\n</code></pre>"},{"location":"api/#nadl.DataLoader.epoch_iter","title":"<code>epoch_iter(epoch_start, epoch_end=None)</code>","text":"<p>Iterate with epochs.</p> Source code in <code>nadl/data.py</code> <pre><code>def epoch_iter(\n  self, epoch_start: int, epoch_end: int | None = None\n) -&gt; Iterator[DState[T]]:\n  \"\"\"Iterate with epochs.\"\"\"\n  gen = filter_jit(filter_vmap(self.gen))\n  # steps = jnp.arange(self._length)\n  if epoch_end is None:\n    epoch_end = epoch_start + 1\n    epoch_start = 1\n  data = gen(jnp.arange(epoch_start, epoch_end))\n\n  data = jax.tree.map(self._ex, data)\n  for step, x, pad, length, epoch, key in zip(\n    jnp.arange(data.xs.shape[0]) + 1,\n    data.xs,\n    data.pad,\n    data.length,\n    data.epoch,\n    data.key,\n  ):\n    yield DState(\n      self.transform(self.embed(x, key=key), key=key),\n      pad,\n      length,\n      epoch,\n      step,\n      key,\n    )\n</code></pre>"},{"location":"api/#nadl.DataLoader.viter","title":"<code>viter(epochs, chunks=100, *, epoch_bias=1)</code>","text":"<p>Iterate with epochs and chunks.</p> Source code in <code>nadl/data.py</code> <pre><code>def viter(\n  self, epochs: int, chunks: int = 100, *, epoch_bias: int = 1\n) -&gt; Iterator[DState[T]]:\n  \"\"\"Iterate with epochs and chunks.\"\"\"\n  for chunk in range(epoch_bias, epochs + epoch_bias, chunks):\n    yield from self.epoch_iter(chunk, min(chunk + chunks, epochs + epoch_bias))\n</code></pre>"},{"location":"api/#nadl.FastKAN","title":"<code>FastKAN</code>","text":"<p>               Bases: <code>Module</code></p> <p>FastKAN.</p> <p>FastKAN: Very Fast Implementation (Approximation) of Kolmogorov-Arnold Network.</p> Source code in <code>nadl/nets.py</code> <pre><code>class FastKAN(eqx.Module):\n  \"\"\"FastKAN.\n\n  FastKAN: Very Fast Implementation (Approximation) of Kolmogorov-Arnold Network.\n  \"\"\"\n\n  layers: eqx.nn.Sequential\n\n  def __init__(\n    self,\n    layers_hidden: list[int],\n    grid_min: float = -2.0,\n    grid_max: float = 2.0,\n    num_grids: int = 8,\n    use_base_update: bool = True,\n    base_activation: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]] = jax.nn.silu,\n    spline_weight_init_scale: float = 0.1,\n    *,\n    key: PRNGKeyArray,\n  ) -&gt; None:\n    \"\"\"Initialize the FastKAN.\"\"\"\n    ks = jax.random.split(key, len(layers_hidden) - 1)\n    self.layers = eqx.nn.Sequential([\n      FastKANLayer(\n        in_dim,\n        out_dim,\n        grid_min=grid_min,\n        grid_max=grid_max,\n        num_grids=num_grids,\n        use_base_update=use_base_update,\n        base_activation=base_activation,\n        spline_weight_init_scale=spline_weight_init_scale,\n        key=k,\n      )\n      for in_dim, out_dim, k in zip(\n        layers_hidden[:-1], layers_hidden[1:], ks, strict=True\n      )\n    ])\n\n  def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n    \"\"\"Forward.\"\"\"\n    return self.layers(x)\n</code></pre>"},{"location":"api/#nadl.FastKAN.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward.</p> Source code in <code>nadl/nets.py</code> <pre><code>def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n  \"\"\"Forward.\"\"\"\n  return self.layers(x)\n</code></pre>"},{"location":"api/#nadl.FastKAN.__init__","title":"<code>__init__(layers_hidden, grid_min=-2.0, grid_max=2.0, num_grids=8, use_base_update=True, base_activation=jax.nn.silu, spline_weight_init_scale=0.1, *, key)</code>","text":"<p>Initialize the FastKAN.</p> Source code in <code>nadl/nets.py</code> <pre><code>def __init__(\n  self,\n  layers_hidden: list[int],\n  grid_min: float = -2.0,\n  grid_max: float = 2.0,\n  num_grids: int = 8,\n  use_base_update: bool = True,\n  base_activation: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]] = jax.nn.silu,\n  spline_weight_init_scale: float = 0.1,\n  *,\n  key: PRNGKeyArray,\n) -&gt; None:\n  \"\"\"Initialize the FastKAN.\"\"\"\n  ks = jax.random.split(key, len(layers_hidden) - 1)\n  self.layers = eqx.nn.Sequential([\n    FastKANLayer(\n      in_dim,\n      out_dim,\n      grid_min=grid_min,\n      grid_max=grid_max,\n      num_grids=num_grids,\n      use_base_update=use_base_update,\n      base_activation=base_activation,\n      spline_weight_init_scale=spline_weight_init_scale,\n      key=k,\n    )\n    for in_dim, out_dim, k in zip(\n      layers_hidden[:-1], layers_hidden[1:], ks, strict=True\n    )\n  ])\n</code></pre>"},{"location":"api/#nadl.FastKANLayer","title":"<code>FastKANLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Gaussian Radial Basis Functions replace the spline fast KAN.</p> <p>From https://github.com/ZiyaoLi/fast-kan/blob/master/fastkan/fastkan.py</p> Source code in <code>nadl/blocks.py</code> <pre><code>class FastKANLayer(eqx.Module):\n  \"\"\"Gaussian Radial Basis Functions replace the spline fast KAN.\n\n  From https://github.com/ZiyaoLi/fast-kan/blob/master/fastkan/fastkan.py\n  \"\"\"\n\n  rbf: RadialBasisFunction\n  spline_linear: SplineLinear\n  use_base_update: bool\n  use_layernorm: bool\n  layernorm: eqx.nn.LayerNorm | None = None\n  base_activation: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]] = jax.nn.silu\n  base_linear: eqx.nn.Linear | None = None\n\n  def __init__(  # noqa: PLR0913\n    self,\n    input_dim: int,\n    output_dim: int,\n    grid_min: float = -2.0,\n    grid_max: float = 2.0,\n    num_grids: int = 5,\n    use_base_update: bool = True,\n    use_layernorm: bool = True,\n    base_activation: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]] = jax.nn.silu,\n    spline_weight_init_scale: float = 0.1,\n    *,\n    key: PRNGKeyArray,\n  ) -&gt; None:\n    \"\"\"Initialize.\"\"\"\n    self.use_layernorm = use_layernorm\n    if use_layernorm:\n      assert (\n        input_dim &gt; 1\n      ), \"Do not use layernorms on 1D inputs. Set `use_layernorm=False`.\"\n      self.layernorm = eqx.nn.LayerNorm(input_dim)\n\n    self.rbf = RadialBasisFunction(grid_min, grid_max, num_grids)\n\n    self.spline_linear = SplineLinear(\n      input_dim * num_grids, output_dim, spline_weight_init_scale, key=key\n    )\n\n    self.use_base_update = use_base_update\n    if use_base_update:\n      self.base_activation = base_activation\n      self.base_linear = eqx.nn.Linear(input_dim, output_dim, key=key)\n\n  def __call__(\n    self, x: Float[Array, \" A\"]\n  ) -&gt; Float[Array, \" A\"]:\n    \"\"\"Forward.\"\"\"\n    if self.use_layernorm:\n      assert self.layernorm is not None, \"Layernorm is not initialized.\"\n      x = self.layernorm(x)\n    spline_basis = self.rbf(x)\n\n    ret = self.spline_linear(spline_basis.reshape(*spline_basis.shape[:-2], -1))\n\n    if self.use_base_update:\n      assert self.base_linear is not None, \"Base linear is not initialized.\"\n      base = self.base_linear(self.base_activation(x))\n      ret += base\n\n    return ret\n\n  @property\n  def input_dim(self) -&gt; int:\n    \"\"\"Get input dimension.\"\"\"\n    return self.spline_linear.linear.weight.shape[1] // self.rbf.num_grids\n\n  @property\n  def output_dim(self) -&gt; int:\n    \"\"\"Get output dimension.\"\"\"\n    return self.spline_linear.linear.weight.shape[0]\n\n  def plot_curve(\n    self,\n    input_index: int,\n    output_index: int,\n    num_pts: int = 1000,\n    num_extrapolate_bins: int = 2,\n  ) -&gt; tuple[Float[Array, \" A\"], Float[Array, \" A\"]]:\n    \"\"\"This function returns the learned curves in a FastKANLayer.\n\n    input_index: the selected index of the input, in [0, input_dim) .\n    output_index: the selected index of the output, in [0, output_dim) .\n    num_pts: num of points sampled for the curve.\n    num_extrapolate_bins (N_e): num of bins extrapolating from the given grids. The\n        curve will be calculate in the range of\n        [grid_min - h * N_e, grid_max + h * N_e].\n    \"\"\"  # noqa: D404\n    ng = self.rbf.num_grids\n    h = self.rbf.denominator\n    assert input_index &lt; self.input_dim\n    assert output_index &lt; self.output_dim\n    w = self.spline_linear.linear.weight[\n      output_index, input_index * ng : (input_index + 1) * ng\n    ]  # num_grids,\n    x = jnp.linspace(\n      self.rbf.grid_min - num_extrapolate_bins * h,\n      self.rbf.grid_max + num_extrapolate_bins * h,\n      num_pts,\n    )  # num_pts, num_grids\n    y = (w * self.rbf(x)).sum(-1)\n    return x, y\n</code></pre>"},{"location":"api/#nadl.FastKANLayer.input_dim","title":"<code>input_dim: int</code>  <code>property</code>","text":"<p>Get input dimension.</p>"},{"location":"api/#nadl.FastKANLayer.output_dim","title":"<code>output_dim: int</code>  <code>property</code>","text":"<p>Get output dimension.</p>"},{"location":"api/#nadl.FastKANLayer.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward.</p> Source code in <code>nadl/blocks.py</code> <pre><code>def __call__(\n  self, x: Float[Array, \" A\"]\n) -&gt; Float[Array, \" A\"]:\n  \"\"\"Forward.\"\"\"\n  if self.use_layernorm:\n    assert self.layernorm is not None, \"Layernorm is not initialized.\"\n    x = self.layernorm(x)\n  spline_basis = self.rbf(x)\n\n  ret = self.spline_linear(spline_basis.reshape(*spline_basis.shape[:-2], -1))\n\n  if self.use_base_update:\n    assert self.base_linear is not None, \"Base linear is not initialized.\"\n    base = self.base_linear(self.base_activation(x))\n    ret += base\n\n  return ret\n</code></pre>"},{"location":"api/#nadl.FastKANLayer.__init__","title":"<code>__init__(input_dim, output_dim, grid_min=-2.0, grid_max=2.0, num_grids=5, use_base_update=True, use_layernorm=True, base_activation=jax.nn.silu, spline_weight_init_scale=0.1, *, key)</code>","text":"<p>Initialize.</p> Source code in <code>nadl/blocks.py</code> <pre><code>def __init__(  # noqa: PLR0913\n  self,\n  input_dim: int,\n  output_dim: int,\n  grid_min: float = -2.0,\n  grid_max: float = 2.0,\n  num_grids: int = 5,\n  use_base_update: bool = True,\n  use_layernorm: bool = True,\n  base_activation: Callable[[Float[Array, \"...\"]], Float[Array, \"...\"]] = jax.nn.silu,\n  spline_weight_init_scale: float = 0.1,\n  *,\n  key: PRNGKeyArray,\n) -&gt; None:\n  \"\"\"Initialize.\"\"\"\n  self.use_layernorm = use_layernorm\n  if use_layernorm:\n    assert (\n      input_dim &gt; 1\n    ), \"Do not use layernorms on 1D inputs. Set `use_layernorm=False`.\"\n    self.layernorm = eqx.nn.LayerNorm(input_dim)\n\n  self.rbf = RadialBasisFunction(grid_min, grid_max, num_grids)\n\n  self.spline_linear = SplineLinear(\n    input_dim * num_grids, output_dim, spline_weight_init_scale, key=key\n  )\n\n  self.use_base_update = use_base_update\n  if use_base_update:\n    self.base_activation = base_activation\n    self.base_linear = eqx.nn.Linear(input_dim, output_dim, key=key)\n</code></pre>"},{"location":"api/#nadl.FastKANLayer.plot_curve","title":"<code>plot_curve(input_index, output_index, num_pts=1000, num_extrapolate_bins=2)</code>","text":"<p>This function returns the learned curves in a FastKANLayer.</p> <p>input_index: the selected index of the input, in [0, input_dim) . output_index: the selected index of the output, in [0, output_dim) . num_pts: num of points sampled for the curve. num_extrapolate_bins (N_e): num of bins extrapolating from the given grids. The     curve will be calculate in the range of     [grid_min - h * N_e, grid_max + h * N_e].</p> Source code in <code>nadl/blocks.py</code> <pre><code>def plot_curve(\n  self,\n  input_index: int,\n  output_index: int,\n  num_pts: int = 1000,\n  num_extrapolate_bins: int = 2,\n) -&gt; tuple[Float[Array, \" A\"], Float[Array, \" A\"]]:\n  \"\"\"This function returns the learned curves in a FastKANLayer.\n\n  input_index: the selected index of the input, in [0, input_dim) .\n  output_index: the selected index of the output, in [0, output_dim) .\n  num_pts: num of points sampled for the curve.\n  num_extrapolate_bins (N_e): num of bins extrapolating from the given grids. The\n      curve will be calculate in the range of\n      [grid_min - h * N_e, grid_max + h * N_e].\n  \"\"\"  # noqa: D404\n  ng = self.rbf.num_grids\n  h = self.rbf.denominator\n  assert input_index &lt; self.input_dim\n  assert output_index &lt; self.output_dim\n  w = self.spline_linear.linear.weight[\n    output_index, input_index * ng : (input_index + 1) * ng\n  ]  # num_grids,\n  x = jnp.linspace(\n    self.rbf.grid_min - num_extrapolate_bins * h,\n    self.rbf.grid_max + num_extrapolate_bins * h,\n    num_pts,\n  )  # num_pts, num_grids\n  y = (w * self.rbf(x)).sum(-1)\n  return x, y\n</code></pre>"},{"location":"api/#nadl.Keys","title":"<code>Keys</code>","text":"<p>               Bases: <code>Module</code></p> <p>JAX random key sequence.</p> <p>init_key:  The initial key. keys: The key sequence in cluding the histories. idx: Current key idx in the key sequence.</p> Source code in <code>nadl/keys.py</code> <pre><code>class Keys(Module):\n  \"\"\"JAX random key sequence.\n\n  init_key:  The initial key.\n  keys: The key sequence in cluding the histories.\n  idx: Current key idx in the key sequence.\n  \"\"\"\n\n  init_key: PRNGKeyArray\n  keys: PRNGKeyArray\n  idx: ScalarLike = 0\n\n  def __call__(self, epoch: ArrayLike) -&gt; jax.Array:\n    \"\"\"Get keys for epoch.\"\"\"\n    return jax.random.fold_in(self.init_key, epoch)\n\n  def __iter__(self) -&gt; Keys:\n    \"\"\"Iterate the keys.\"\"\"\n    return self\n\n  def __next__(self) -&gt; Keys:\n    \"\"\"Get next key.\"\"\"\n    return self.next_key()\n\n  def __len__(self) -&gt; int:\n    \"\"\"Length of keys.\"\"\"\n    return self.keys.shape[0]\n\n  @property\n  def key(self) -&gt; PRNGKeyArray:\n    \"\"\"Get key.\"\"\"\n    if self.keys.shape[0]:\n      return self.keys[-1]\n    return self(self.idx + 1)\n\n  @property\n  def state(self) -&gt; tuple[PRNGKeyArray, PRNGKeyArray, ScalarLike]:\n    \"\"\"Get state.\"\"\"\n    return self.init_key, self.keys, self.idx\n\n  @classmethod\n  def from_int_or_key(cls: type[Keys], key: PRNGKeyArray | int) -&gt; Keys:\n    \"\"\"Convert int or key to Keys.\"\"\"\n    if isinstance(key, int):\n      key = jax.random.key(key)\n    return cls(key, jax.random.fold_in(key, 0).reshape(-1), jnp.asarray(0))\n\n  @classmethod\n  def from_state(\n    cls: type[Keys], key: PRNGKeyArray, keys: PRNGKeyArray, idx: ScalarLike\n  ) -&gt; Keys:\n    \"\"\"Convert state to Keys.\"\"\"\n    return cls(key, keys, idx)\n\n  def reserve(self, num: int | jax.Array) -&gt; Keys:\n    \"\"\"Reverse the keys.\"\"\"\n    return tree_at(\n      lambda x: x.keys,\n      self,\n      jax.vmap(self)(jnp.arange(jnp.maximum(num, len(self)))),\n    )\n\n  def next_key(self) -&gt; Keys:\n    \"\"\"Get next key.\"\"\"\n    return tree_at(lambda k: k.idx, self, self.idx + 1)\n\n  def take(self, num: int) -&gt; PRNGKeyArray:\n    \"\"\"Take num keys.\"\"\"\n    return jax.vmap(self)(jnp.arange(num)) if num &gt; len(self) else self.keys[-num:]\n</code></pre>"},{"location":"api/#nadl.Keys.key","title":"<code>key: PRNGKeyArray</code>  <code>property</code>","text":"<p>Get key.</p>"},{"location":"api/#nadl.Keys.state","title":"<code>state: tuple[PRNGKeyArray, PRNGKeyArray, ScalarLike]</code>  <code>property</code>","text":"<p>Get state.</p>"},{"location":"api/#nadl.Keys.__call__","title":"<code>__call__(epoch)</code>","text":"<p>Get keys for epoch.</p> Source code in <code>nadl/keys.py</code> <pre><code>def __call__(self, epoch: ArrayLike) -&gt; jax.Array:\n  \"\"\"Get keys for epoch.\"\"\"\n  return jax.random.fold_in(self.init_key, epoch)\n</code></pre>"},{"location":"api/#nadl.Keys.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate the keys.</p> Source code in <code>nadl/keys.py</code> <pre><code>def __iter__(self) -&gt; Keys:\n  \"\"\"Iterate the keys.\"\"\"\n  return self\n</code></pre>"},{"location":"api/#nadl.Keys.__len__","title":"<code>__len__()</code>","text":"<p>Length of keys.</p> Source code in <code>nadl/keys.py</code> <pre><code>def __len__(self) -&gt; int:\n  \"\"\"Length of keys.\"\"\"\n  return self.keys.shape[0]\n</code></pre>"},{"location":"api/#nadl.Keys.__next__","title":"<code>__next__()</code>","text":"<p>Get next key.</p> Source code in <code>nadl/keys.py</code> <pre><code>def __next__(self) -&gt; Keys:\n  \"\"\"Get next key.\"\"\"\n  return self.next_key()\n</code></pre>"},{"location":"api/#nadl.Keys.from_int_or_key","title":"<code>from_int_or_key(key)</code>  <code>classmethod</code>","text":"<p>Convert int or key to Keys.</p> Source code in <code>nadl/keys.py</code> <pre><code>@classmethod\ndef from_int_or_key(cls: type[Keys], key: PRNGKeyArray | int) -&gt; Keys:\n  \"\"\"Convert int or key to Keys.\"\"\"\n  if isinstance(key, int):\n    key = jax.random.key(key)\n  return cls(key, jax.random.fold_in(key, 0).reshape(-1), jnp.asarray(0))\n</code></pre>"},{"location":"api/#nadl.Keys.from_state","title":"<code>from_state(key, keys, idx)</code>  <code>classmethod</code>","text":"<p>Convert state to Keys.</p> Source code in <code>nadl/keys.py</code> <pre><code>@classmethod\ndef from_state(\n  cls: type[Keys], key: PRNGKeyArray, keys: PRNGKeyArray, idx: ScalarLike\n) -&gt; Keys:\n  \"\"\"Convert state to Keys.\"\"\"\n  return cls(key, keys, idx)\n</code></pre>"},{"location":"api/#nadl.Keys.next_key","title":"<code>next_key()</code>","text":"<p>Get next key.</p> Source code in <code>nadl/keys.py</code> <pre><code>def next_key(self) -&gt; Keys:\n  \"\"\"Get next key.\"\"\"\n  return tree_at(lambda k: k.idx, self, self.idx + 1)\n</code></pre>"},{"location":"api/#nadl.Keys.reserve","title":"<code>reserve(num)</code>","text":"<p>Reverse the keys.</p> Source code in <code>nadl/keys.py</code> <pre><code>def reserve(self, num: int | jax.Array) -&gt; Keys:\n  \"\"\"Reverse the keys.\"\"\"\n  return tree_at(\n    lambda x: x.keys,\n    self,\n    jax.vmap(self)(jnp.arange(jnp.maximum(num, len(self)))),\n  )\n</code></pre>"},{"location":"api/#nadl.Keys.take","title":"<code>take(num)</code>","text":"<p>Take num keys.</p> Source code in <code>nadl/keys.py</code> <pre><code>def take(self, num: int) -&gt; PRNGKeyArray:\n  \"\"\"Take num keys.\"\"\"\n  return jax.vmap(self)(jnp.arange(num)) if num &gt; len(self) else self.keys[-num:]\n</code></pre>"},{"location":"api/#nadl.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base Metric.</p> <p>This metric is for batch data not for a single one. Thus, please consider the shape of the input data as [b, ...].</p> <p>The default getitem will only take the array with ndim &gt; 1.</p> Source code in <code>nadl/metrics.py</code> <pre><code>class Metric[**P, T](Module):\n  \"\"\"Base Metric.\n\n  This metric is for batch data not for a single one.\n  Thus, please consider the shape of the input data as [b, ...].\n\n  The default getitem will only take the array with ndim &gt; 1.\n  \"\"\"\n\n  name: AbstractVar[str | None]\n\n  def __post_init__(self) -&gt; None:\n    \"\"\"Post init.\"\"\"\n    for k, v in self.__dict__.items():\n      if isinstance(v, Array | np.ndarray):\n        self.__dict__[k] = convert(v)\n\n  def __check_init__(self) -&gt; None:  # noqa: PLW3201\n    \"\"\"Check init.\"\"\"\n    arrs = filter_tree(self, batch_array_p)\n    if arrs:\n      if not tree_equal(*jax.tree.map(lambda x: jnp.shape(x)[0], arrs)):\n        raise ValueError(\"All batched array should have the same batch size.\")\n    else:\n      warn(\"No batched array found in the metric.\", stacklevel=1)\n\n  @classmethod\n  def merge(cls, *metrics: Self) -&gt; Self:\n    \"\"\"Merge all metrics.\"\"\"\n    _, s2 = partition(metrics, is_array)\n    if not tree_equal(*s2):\n      raise ValueError(\"All metrics should have the same non-array values.\")\n    return filter_concat(metrics, batch_array_p)\n\n  def __add__(self, value: Self) -&gt; Self:\n    \"\"\"Add.\"\"\"\n    return self.merge(self, value)\n\n  def __or__(self, value: Self) -&gt; Self:\n    \"\"\"Or.\"\"\"\n    return combine(self, value)\n\n  @classmethod\n  def empty(cls, *_args: P.args, **_kwds: P.kwargs) -&gt; Self:\n    \"\"\"Empty.\"\"\"\n    with catch_warnings():\n      simplefilter(\"ignore\")\n      return cls(**dict.fromkeys(cls.__dataclass_fields__))\n\n  def compute(self) -&gt; T:\n    \"\"\"Compute.\"\"\"\n    raise NotImplementedError\n\n  def __getitem__(self, idx: int | slice | Int[ArrayLike, \"...\"]) -&gt; Self:\n    \"\"\"Get item.\n\n    The default getitem will only take the array with ndim &gt; 1.\n    \"\"\"\n    return jax.tree.map(lambda x: x[idx] if batch_array_p(x) else x, self)\n\n  def show(self) -&gt; str:\n    \"\"\"Show.\"\"\"\n    return tree_pformat(self, short_arrays=False)\n</code></pre>"},{"location":"api/#nadl.Metric.__add__","title":"<code>__add__(value)</code>","text":"<p>Add.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def __add__(self, value: Self) -&gt; Self:\n  \"\"\"Add.\"\"\"\n  return self.merge(self, value)\n</code></pre>"},{"location":"api/#nadl.Metric.__check_init__","title":"<code>__check_init__()</code>","text":"<p>Check init.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def __check_init__(self) -&gt; None:  # noqa: PLW3201\n  \"\"\"Check init.\"\"\"\n  arrs = filter_tree(self, batch_array_p)\n  if arrs:\n    if not tree_equal(*jax.tree.map(lambda x: jnp.shape(x)[0], arrs)):\n      raise ValueError(\"All batched array should have the same batch size.\")\n  else:\n    warn(\"No batched array found in the metric.\", stacklevel=1)\n</code></pre>"},{"location":"api/#nadl.Metric.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get item.</p> <p>The default getitem will only take the array with ndim &gt; 1.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def __getitem__(self, idx: int | slice | Int[ArrayLike, \"...\"]) -&gt; Self:\n  \"\"\"Get item.\n\n  The default getitem will only take the array with ndim &gt; 1.\n  \"\"\"\n  return jax.tree.map(lambda x: x[idx] if batch_array_p(x) else x, self)\n</code></pre>"},{"location":"api/#nadl.Metric.__or__","title":"<code>__or__(value)</code>","text":"<p>Or.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def __or__(self, value: Self) -&gt; Self:\n  \"\"\"Or.\"\"\"\n  return combine(self, value)\n</code></pre>"},{"location":"api/#nadl.Metric.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post init.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def __post_init__(self) -&gt; None:\n  \"\"\"Post init.\"\"\"\n  for k, v in self.__dict__.items():\n    if isinstance(v, Array | np.ndarray):\n      self.__dict__[k] = convert(v)\n</code></pre>"},{"location":"api/#nadl.Metric.compute","title":"<code>compute()</code>","text":"<p>Compute.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def compute(self) -&gt; T:\n  \"\"\"Compute.\"\"\"\n  raise NotImplementedError\n</code></pre>"},{"location":"api/#nadl.Metric.empty","title":"<code>empty(*_args, **_kwds)</code>  <code>classmethod</code>","text":"<p>Empty.</p> Source code in <code>nadl/metrics.py</code> <pre><code>@classmethod\ndef empty(cls, *_args: P.args, **_kwds: P.kwargs) -&gt; Self:\n  \"\"\"Empty.\"\"\"\n  with catch_warnings():\n    simplefilter(\"ignore\")\n    return cls(**dict.fromkeys(cls.__dataclass_fields__))\n</code></pre>"},{"location":"api/#nadl.Metric.merge","title":"<code>merge(*metrics)</code>  <code>classmethod</code>","text":"<p>Merge all metrics.</p> Source code in <code>nadl/metrics.py</code> <pre><code>@classmethod\ndef merge(cls, *metrics: Self) -&gt; Self:\n  \"\"\"Merge all metrics.\"\"\"\n  _, s2 = partition(metrics, is_array)\n  if not tree_equal(*s2):\n    raise ValueError(\"All metrics should have the same non-array values.\")\n  return filter_concat(metrics, batch_array_p)\n</code></pre>"},{"location":"api/#nadl.Metric.show","title":"<code>show()</code>","text":"<p>Show.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def show(self) -&gt; str:\n  \"\"\"Show.\"\"\"\n  return tree_pformat(self, short_arrays=False)\n</code></pre>"},{"location":"api/#nadl.PG","title":"<code>PG</code>","text":"<p>               Bases: <code>Module</code></p> <p>Progress.</p> Source code in <code>nadl/loops.py</code> <pre><code>class PG(Module):\n  \"\"\"Progress.\"\"\"\n\n  pg: Progress\n  console: Console\n  tasks: dict[Hashable, TaskID]\n\n  @classmethod\n  def init_progress(\n    cls: type[Self],\n    pg: Progress | None = None,\n    console: Console | None = None,\n    total: bool = True,\n    bar_width: int | None = 20,\n    extra_columns: tuple[ProgressColumn, ...] = (),\n    show_progress: bool = True,\n    theme: Theme | None = None,\n  ) -&gt; Self:\n    \"\"\"Init progress bar.\"\"\"\n    if console is None:\n      console = Console(theme=theme or DEF_LIGHT_THEME)\n    if pg is None:\n      pg = Progress(\n        TextColumn(\n          \"{task.description}\" + \" - {task.completed}/{task.total}\" if total else \"\"\n        ),\n        TimeRemainingColumn(),\n        TimeElapsedColumn(),\n        BarColumn(bar_width),\n        console=console,\n        disable=not show_progress,\n      )\n    pg.columns += extra_columns\n    return cls(pg, console, {})\n\n  def add_columns(self, columns: tuple[ProgressColumn, ...]) -&gt; Self:\n    \"\"\"Add columns.\"\"\"\n    self.pg.columns += columns\n    return self\n\n  def add_task(\n    self,\n    description: str,\n    start: bool = True,\n    total: float | None = 100,\n    completed: int = 0,\n    visible: bool = True,\n    **fileds: Any,  # noqa: ANN401\n  ) -&gt; TaskID:\n    \"\"\"Add task.\"\"\"\n    task = self.pg.add_task(\n      description,\n      start=start,\n      total=total,\n      completed=completed,\n      visible=visible,\n      **fileds,\n    )\n    self.tasks[description] = task\n    return task\n\n  def advance(self, task: TaskID, advance: float = 1) -&gt; None:\n    \"\"\"Advance task.\"\"\"\n    self.pg.advance(task, advance=advance)\n\n  def __enter__(self) -&gt; Progress:\n    \"\"\"Enter.\"\"\"\n    return self.pg.__enter__()\n\n  def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n  ) -&gt; None:\n    \"\"\"Exit.\"\"\"\n    self.pg.__exit__(exc_type, exc_val, exc_tb)\n\n  def update_res(\n    self, name: str, updates: Mapping[str, float | int | str | None]\n  ) -&gt; None:\n    \"\"\"Update res.\"\"\"\n    if name in self.tasks:\n      self.pg.update(self.tasks[name], res=pformat(updates))\n</code></pre>"},{"location":"api/#nadl.PG.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter.</p> Source code in <code>nadl/loops.py</code> <pre><code>def __enter__(self) -&gt; Progress:\n  \"\"\"Enter.\"\"\"\n  return self.pg.__enter__()\n</code></pre>"},{"location":"api/#nadl.PG.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit.</p> Source code in <code>nadl/loops.py</code> <pre><code>def __exit__(\n  self,\n  exc_type: type[BaseException] | None,\n  exc_val: BaseException | None,\n  exc_tb: TracebackType | None,\n) -&gt; None:\n  \"\"\"Exit.\"\"\"\n  self.pg.__exit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"api/#nadl.PG.add_columns","title":"<code>add_columns(columns)</code>","text":"<p>Add columns.</p> Source code in <code>nadl/loops.py</code> <pre><code>def add_columns(self, columns: tuple[ProgressColumn, ...]) -&gt; Self:\n  \"\"\"Add columns.\"\"\"\n  self.pg.columns += columns\n  return self\n</code></pre>"},{"location":"api/#nadl.PG.add_task","title":"<code>add_task(description, start=True, total=100, completed=0, visible=True, **fileds)</code>","text":"<p>Add task.</p> Source code in <code>nadl/loops.py</code> <pre><code>def add_task(\n  self,\n  description: str,\n  start: bool = True,\n  total: float | None = 100,\n  completed: int = 0,\n  visible: bool = True,\n  **fileds: Any,  # noqa: ANN401\n) -&gt; TaskID:\n  \"\"\"Add task.\"\"\"\n  task = self.pg.add_task(\n    description,\n    start=start,\n    total=total,\n    completed=completed,\n    visible=visible,\n    **fileds,\n  )\n  self.tasks[description] = task\n  return task\n</code></pre>"},{"location":"api/#nadl.PG.advance","title":"<code>advance(task, advance=1)</code>","text":"<p>Advance task.</p> Source code in <code>nadl/loops.py</code> <pre><code>def advance(self, task: TaskID, advance: float = 1) -&gt; None:\n  \"\"\"Advance task.\"\"\"\n  self.pg.advance(task, advance=advance)\n</code></pre>"},{"location":"api/#nadl.PG.init_progress","title":"<code>init_progress(pg=None, console=None, total=True, bar_width=20, extra_columns=(), show_progress=True, theme=None)</code>  <code>classmethod</code>","text":"<p>Init progress bar.</p> Source code in <code>nadl/loops.py</code> <pre><code>@classmethod\ndef init_progress(\n  cls: type[Self],\n  pg: Progress | None = None,\n  console: Console | None = None,\n  total: bool = True,\n  bar_width: int | None = 20,\n  extra_columns: tuple[ProgressColumn, ...] = (),\n  show_progress: bool = True,\n  theme: Theme | None = None,\n) -&gt; Self:\n  \"\"\"Init progress bar.\"\"\"\n  if console is None:\n    console = Console(theme=theme or DEF_LIGHT_THEME)\n  if pg is None:\n    pg = Progress(\n      TextColumn(\n        \"{task.description}\" + \" - {task.completed}/{task.total}\" if total else \"\"\n      ),\n      TimeRemainingColumn(),\n      TimeElapsedColumn(),\n      BarColumn(bar_width),\n      console=console,\n      disable=not show_progress,\n    )\n  pg.columns += extra_columns\n  return cls(pg, console, {})\n</code></pre>"},{"location":"api/#nadl.PG.update_res","title":"<code>update_res(name, updates)</code>","text":"<p>Update res.</p> Source code in <code>nadl/loops.py</code> <pre><code>def update_res(\n  self, name: str, updates: Mapping[str, float | int | str | None]\n) -&gt; None:\n  \"\"\"Update res.\"\"\"\n  if name in self.tasks:\n    self.pg.update(self.tasks[name], res=pformat(updates))\n</code></pre>"},{"location":"api/#nadl.PGThread","title":"<code>PGThread</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Progress thread.</p> Source code in <code>nadl/loops.py</code> <pre><code>class PGThread(Thread):\n  \"\"\"Progress thread.\"\"\"\n\n  def __init__(self, pg: Progress, task_id: TaskID) -&gt; None:\n    \"\"\"Init.\"\"\"\n    super().__init__()\n    self.pg = pg\n    self.tid = task_id\n    self.done = Event()\n    self.completed = 0\n    self.res: str | None = None\n    super().__init__()\n\n  def run(self) -&gt; None:\n    \"\"\"Run.\"\"\"\n    tid = self.tid\n    last_completed = 0\n    wait = self.done.wait\n    advance = self.pg.advance\n    update = self.pg.update\n    while not wait(0.2):\n      if (completed := self.completed) != last_completed:\n        advance(self.tid, completed - last_completed)\n        last_completed = completed\n        if self.res is not None:\n          update(tid, res=self.res)\n          self.res = None\n    update(self.tid, completed=self.completed, refresh=True)\n\n  def __enter__(self) -&gt; Self:\n    \"\"\"Enter.\"\"\"\n    self.start()\n    return self\n\n  def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n  ) -&gt; None:\n    \"\"\"Exit.\"\"\"\n    del exc_type, exc_val, exc_tb\n    self.done.set()\n    self.join()\n</code></pre>"},{"location":"api/#nadl.PGThread.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter.</p> Source code in <code>nadl/loops.py</code> <pre><code>def __enter__(self) -&gt; Self:\n  \"\"\"Enter.\"\"\"\n  self.start()\n  return self\n</code></pre>"},{"location":"api/#nadl.PGThread.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit.</p> Source code in <code>nadl/loops.py</code> <pre><code>def __exit__(\n  self,\n  exc_type: type[BaseException] | None,\n  exc_val: BaseException | None,\n  exc_tb: TracebackType | None,\n) -&gt; None:\n  \"\"\"Exit.\"\"\"\n  del exc_type, exc_val, exc_tb\n  self.done.set()\n  self.join()\n</code></pre>"},{"location":"api/#nadl.PGThread.__init__","title":"<code>__init__(pg, task_id)</code>","text":"<p>Init.</p> Source code in <code>nadl/loops.py</code> <pre><code>def __init__(self, pg: Progress, task_id: TaskID) -&gt; None:\n  \"\"\"Init.\"\"\"\n  super().__init__()\n  self.pg = pg\n  self.tid = task_id\n  self.done = Event()\n  self.completed = 0\n  self.res: str | None = None\n  super().__init__()\n</code></pre>"},{"location":"api/#nadl.PGThread.run","title":"<code>run()</code>","text":"<p>Run.</p> Source code in <code>nadl/loops.py</code> <pre><code>def run(self) -&gt; None:\n  \"\"\"Run.\"\"\"\n  tid = self.tid\n  last_completed = 0\n  wait = self.done.wait\n  advance = self.pg.advance\n  update = self.pg.update\n  while not wait(0.2):\n    if (completed := self.completed) != last_completed:\n      advance(self.tid, completed - last_completed)\n      last_completed = completed\n      if self.res is not None:\n        update(tid, res=self.res)\n        self.res = None\n  update(self.tid, completed=self.completed, refresh=True)\n</code></pre>"},{"location":"api/#nadl.RadialBasisFunction","title":"<code>RadialBasisFunction</code>","text":"<p>               Bases: <code>Module</code></p> <p>Gaussian Radial Basis Function.</p> Source code in <code>nadl/blocks.py</code> <pre><code>class RadialBasisFunction(eqx.Module):\n  \"\"\"Gaussian Radial Basis Function.\"\"\"\n\n  grid: Float[Array, \" A\"]\n  num_grids: int\n  grid_min: float\n  grid_max: float\n  denominator: float\n\n  def __init__(\n    self,\n    grid_min: float = -2.0,\n    grid_max: float = 2.0,\n    num_grids: int = 5,\n    denominator: float = 1,  # larger denominators lead to smoother basis\n  ) -&gt; None:\n    \"\"\"Initialize.\"\"\"\n    self.grid = jnp.linspace(grid_min, grid_max, num_grids)\n    self.num_grids = num_grids\n    self.grid_min = grid_min\n    self.grid_max = grid_max\n    self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n\n  def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n    \"\"\"Forward.\"\"\"\n    return jnp.exp(-(((x[..., None] - self.grid) / self.denominator) ** 2))\n</code></pre>"},{"location":"api/#nadl.RadialBasisFunction.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward.</p> Source code in <code>nadl/blocks.py</code> <pre><code>def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n  \"\"\"Forward.\"\"\"\n  return jnp.exp(-(((x[..., None] - self.grid) / self.denominator) ** 2))\n</code></pre>"},{"location":"api/#nadl.RadialBasisFunction.__init__","title":"<code>__init__(grid_min=-2.0, grid_max=2.0, num_grids=5, denominator=1)</code>","text":"<p>Initialize.</p> Source code in <code>nadl/blocks.py</code> <pre><code>def __init__(\n  self,\n  grid_min: float = -2.0,\n  grid_max: float = 2.0,\n  num_grids: int = 5,\n  denominator: float = 1,  # larger denominators lead to smoother basis\n) -&gt; None:\n  \"\"\"Initialize.\"\"\"\n  self.grid = jnp.linspace(grid_min, grid_max, num_grids)\n  self.num_grids = num_grids\n  self.grid_min = grid_min\n  self.grid_max = grid_max\n  self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n</code></pre>"},{"location":"api/#nadl.SplineLinear","title":"<code>SplineLinear</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spline Linear layer.</p> Source code in <code>nadl/blocks.py</code> <pre><code>class SplineLinear(eqx.Module):\n  \"\"\"Spline Linear layer.\"\"\"\n\n  init_scale: float\n  liner: eqx.nn.Linear\n\n  def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    init_scale: float = 1.0,\n    use_bias: bool = True,\n    *,\n    key: PRNGKeyArray,\n  ) -&gt; None:\n    \"\"\"Initialize.\"\"\"\n    self.init_scale = init_scale\n    linear = eqx.nn.Linear(in_features, out_features, use_bias, key=key)\n    self.linear = eqx.tree_at(\n      lambda ly: ly.weight,\n      linear,\n      jax.nn.initializers.truncated_normal(stddev=init_scale)(key, linear.weight.shape),\n    )\n\n  def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n    \"\"\"Forward.\"\"\"\n    return self.linear(x)\n</code></pre>"},{"location":"api/#nadl.SplineLinear.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward.</p> Source code in <code>nadl/blocks.py</code> <pre><code>def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n  \"\"\"Forward.\"\"\"\n  return self.linear(x)\n</code></pre>"},{"location":"api/#nadl.SplineLinear.__init__","title":"<code>__init__(in_features, out_features, init_scale=1.0, use_bias=True, *, key)</code>","text":"<p>Initialize.</p> Source code in <code>nadl/blocks.py</code> <pre><code>def __init__(\n  self,\n  in_features: int,\n  out_features: int,\n  init_scale: float = 1.0,\n  use_bias: bool = True,\n  *,\n  key: PRNGKeyArray,\n) -&gt; None:\n  \"\"\"Initialize.\"\"\"\n  self.init_scale = init_scale\n  linear = eqx.nn.Linear(in_features, out_features, use_bias, key=key)\n  self.linear = eqx.tree_at(\n    lambda ly: ly.weight,\n    linear,\n    jax.nn.initializers.truncated_normal(stddev=init_scale)(key, linear.weight.shape),\n  )\n</code></pre>"},{"location":"api/#nadl.TransT","title":"<code>TransT</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Transform protocol.</p> Source code in <code>nadl/data.py</code> <pre><code>class TransT[_I, _O](Protocol):\n  \"\"\"Transform protocol.\"\"\"\n\n  def __call__(self, x: _I, *, key: PRNGKeyArray | None) -&gt; _O:\n    \"\"\"Transform Forwardl.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#nadl.TransT.__call__","title":"<code>__call__(x, *, key)</code>","text":"<p>Transform Forwardl.</p> Source code in <code>nadl/data.py</code> <pre><code>def __call__(self, x: _I, *, key: PRNGKeyArray | None) -&gt; _O:\n  \"\"\"Transform Forwardl.\"\"\"\n  ...\n</code></pre>"},{"location":"api/#nadl.pMTnet","title":"<code>pMTnet</code>","text":"<p>               Bases: <code>Module</code></p> <p>pMTnet.</p> <p>https://github.com/tianshilu/pMTnet</p> <p>Deep learning neural network prediction tcr binding specificity to peptide and HLA based on peptide sequences. Please refer to our paper for more details: 'Deep learning-based prediction of T cell receptor-antigen binding specificity.</p> <p>(https://www.nature.com/articles/s42256-021-00383-2)</p> <p>Lu, T., Zhang, Z., Zhu, J. et al. 2021.</p> Source code in <code>nadl/nets.py</code> <pre><code>class pMTnet(eqx.Module):  # noqa: N801\n  \"\"\"pMTnet.\n\n  https://github.com/tianshilu/pMTnet\n\n  Deep learning neural network prediction tcr binding specificity to\n  peptide and HLA based on peptide sequences. Please refer to our\n  paper for more details: 'Deep learning-based prediction of T cell\n  receptor-antigen binding specificity.\n\n  (https://www.nature.com/articles/s42256-021-00383-2)\n\n  Lu, T., Zhang, Z., Zhu, J. et al. 2021.\n  \"\"\"\n\n  layers: eqx.nn.Sequential\n\n  def __init__(\n    self,\n    inp: int,\n    out: int = 1,\n    hiddens: tuple[int, int, int] = (300, 200, 100),\n    dropout_rate: float = 0.2,\n    *,\n    key: PRNGKeyArray,\n  ) -&gt; None:\n    \"\"\"Initialize the pMTnet.\"\"\"\n    k1, k2, k3, k4 = jax.random.split(key, 4)\n    self.layers = eqx.nn.Sequential([\n      eqx.nn.Linear(inp, hiddens[0], key=k1),\n      eqx.nn.Dropout(p=dropout_rate),\n      eqx.nn.Lambda(jax.nn.relu),\n      eqx.nn.Linear(hiddens[0], hiddens[1], key=k2),\n      eqx.nn.Lambda(jax.nn.relu),\n      eqx.nn.Linear(hiddens[1], hiddens[2], key=k3),\n      eqx.nn.Lambda(jax.nn.relu),\n      eqx.nn.Linear(hiddens[2], out, key=k4),\n    ])\n\n  def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n    \"\"\"Forward.\"\"\"\n    return self.layers(x)\n</code></pre>"},{"location":"api/#nadl.pMTnet.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward.</p> Source code in <code>nadl/nets.py</code> <pre><code>def __call__(self, x: Float[Array, \" A\"]) -&gt; Float[Array, \" A\"]:\n  \"\"\"Forward.\"\"\"\n  return self.layers(x)\n</code></pre>"},{"location":"api/#nadl.pMTnet.__init__","title":"<code>__init__(inp, out=1, hiddens=(300, 200, 100), dropout_rate=0.2, *, key)</code>","text":"<p>Initialize the pMTnet.</p> Source code in <code>nadl/nets.py</code> <pre><code>def __init__(\n  self,\n  inp: int,\n  out: int = 1,\n  hiddens: tuple[int, int, int] = (300, 200, 100),\n  dropout_rate: float = 0.2,\n  *,\n  key: PRNGKeyArray,\n) -&gt; None:\n  \"\"\"Initialize the pMTnet.\"\"\"\n  k1, k2, k3, k4 = jax.random.split(key, 4)\n  self.layers = eqx.nn.Sequential([\n    eqx.nn.Linear(inp, hiddens[0], key=k1),\n    eqx.nn.Dropout(p=dropout_rate),\n    eqx.nn.Lambda(jax.nn.relu),\n    eqx.nn.Linear(hiddens[0], hiddens[1], key=k2),\n    eqx.nn.Lambda(jax.nn.relu),\n    eqx.nn.Linear(hiddens[1], hiddens[2], key=k3),\n    eqx.nn.Lambda(jax.nn.relu),\n    eqx.nn.Linear(hiddens[2], out, key=k4),\n  ])\n</code></pre>"},{"location":"api/#nadl.all_array","title":"<code>all_array(x)</code>","text":"<p>All array.</p> Source code in <code>nadl/utils.py</code> <pre><code>def all_array(x: PyTree) -&gt; list[jax.Array]:\n  \"\"\"All array.\"\"\"\n  return filter_tree(x, eqx.is_array)\n</code></pre>"},{"location":"api/#nadl.average_precision_score","title":"<code>average_precision_score(labels, preds, average='macro')</code>","text":"<p>Compute PR.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def average_precision_score(\n  labels: Num[Array, \" A\"],\n  preds: Num[Array, \" A\"],\n  average: Literal[\"micro\", \"macro\"] = \"macro\",\n) -&gt; Scalar:\n  \"\"\"Compute PR.\"\"\"\n  return jax.pure_callback(\n    partial(m.average_precision_score, average=average),\n    jax.ShapeDtypeStruct((), jnp.float32),\n    labels,\n    preds,\n  )\n</code></pre>"},{"location":"api/#nadl.batch_array_p","title":"<code>batch_array_p(x)</code>","text":"<p>Batch array.</p> Source code in <code>nadl/utils.py</code> <pre><code>def batch_array_p(x: jax.Array) -&gt; bool:\n  \"\"\"Batch array.\"\"\"\n  return jnp.ndim(x) &gt; 1\n</code></pre>"},{"location":"api/#nadl.batch_index","title":"<code>batch_index(length, batch_size, drop_last=False, shuffle=False, *, key=None)</code>","text":"<p>Batchify the index.</p> Source code in <code>nadl/data.py</code> <pre><code>def batch_index(\n  length: int,\n  batch_size: int,\n  drop_last: bool = False,\n  shuffle: bool = False,\n  *,\n  key: PRNGKeyArray | None = None,\n) -&gt; _IDX_FN:\n  \"\"\"Batchify the index.\"\"\"\n  new_length = length if not drop_last else length - length % batch_size\n  pad = (batch_size - r) % batch_size if (r := new_length % batch_size) else 0\n  pad = pad if pad != batch_size else 0\n  drop_num = length % batch_size if drop_last else 0\n  _idxes = jnp.arange(length)\n  dlength = jnp.asarray(\n    length // batch_size + (1 if (not drop_num) and length % batch_size else 0)\n  )\n  depoch = jnp.asarray(0)\n  if key is None:\n    warn(\"Key is not provided, using 42 as random key seed.\", stacklevel=1)\n    key = jax.random.key(42)\n\n  @filter_jit\n  def _index(epoch: Int[Array, \"\"] = depoch) -&gt; DState[Int[Array, \" b d\"]]:\n    assert epoch.ndim == 0, \"Epoch must be a scalar.\"\n    new_key = jax.random.fold_in(key, epoch)\n    if shuffle:\n      idxes = jnp.take_along_axis(\n        _idxes,\n        # NOTE: Fallback to numpy argsort since it has performance isssue in CPU.\n        # https://github.com/google/jax/issues/10434\n        fallback_argsort(jax.random.uniform(new_key, (length,))),\n        axis=0,\n      )\n    else:\n      idxes = _idxes\n\n    _len = length if not drop_last else length - drop_num\n    idxes = jnp.r_[idxes, jnp.full(pad, -1, idxes.dtype)]\n    idxes = idxes[: _len + pad].reshape(-1, batch_size)\n    return DState(\n      idxes,\n      jnp.where(idxes == -1, 1, 0).astype(bool),\n      dlength,\n      epoch,\n      key=jax.random.split(new_key, idxes.shape[0]),\n    )\n\n  return _index\n</code></pre>"},{"location":"api/#nadl.classit","title":"<code>classit(x, method='sigmoid', threshold=0.5)</code>","text":"<p>Classify the array.</p> Source code in <code>nadl/utils.py</code> <pre><code>def classit(\n  x: jax.Array,\n  method: Literal[None, \"sigmoid\", \"softmax\", \"threshold\"] = \"sigmoid\",\n  threshold: float = 0.5,\n) -&gt; jax.Array:\n  \"\"\"Classify the array.\"\"\"\n  match method:\n    case \"sigmoid\":\n      return jax.nn.sigmoid(x) &gt; threshold\n    case \"softmax\":\n      x = jax.nn.softmax(x)\n      return jnp.argmax(x, axis=-1, keepdims=True)\n    case \"threshold\":\n      return x &gt; threshold\n    case _:\n      raise ValueError(f\"Unknown method {method}\")\n</code></pre>"},{"location":"api/#nadl.convert","title":"<code>convert(x)</code>","text":"<p>Convert to float.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def convert(x: Num[ArrayLike, \"...\"]) -&gt; Num[Array, \"B ...\"]:\n  \"\"\"Convert to float.\"\"\"\n  x = jnp.asarray(x)\n  return x.reshape(-1, *x.shape[1:])\n</code></pre>"},{"location":"api/#nadl.dice_coef","title":"<code>dice_coef(y_true, y_pred, eps=1e-08)</code>","text":"<p>Compute dice coefficient.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def dice_coef(\n  y_true: Int[Array, \" A\"], y_pred: Int[Array, \" A\"], eps: float = 1e-8\n) -&gt; Scalar:\n  \"\"\"Compute dice coefficient.\"\"\"\n  y_true = jnp.asarray(y_true)\n  y_pred = jnp.asarray(y_pred)\n\n  intersection = jnp.sum(y_true * y_pred)\n  union = jnp.sum(y_true) + jnp.sum(y_pred)\n\n  return (2.0 * intersection) / (union + eps)\n</code></pre>"},{"location":"api/#nadl.es_loop","title":"<code>es_loop(dl, pg, epochs=1, start_epoch=1, chunks=100, prefix='L', es='E', ss='S')</code>","text":"<p>Simple epoch loop.</p> Source code in <code>nadl/data.py</code> <pre><code>def es_loop[T](\n  dl: DataLoader[T],\n  pg: PG,\n  epochs: int = 1,\n  start_epoch: int = 1,\n  chunks: int = 100,\n  prefix: str = \"L\",\n  es: str = \"E\",\n  ss: str = \"S\",\n) -&gt; Iterator[DState[T]]:\n  \"\"\"Simple epoch loop.\"\"\"\n  assert epochs &gt; 0, \"Epochs should be greater than 0.\"\n  es, ss = f\"{prefix}-{es}\", f\"{prefix}-{ss}\"\n  if es in pg.tasks:\n    pg.pg.reset(pg.tasks[es], total=epochs)\n  else:\n    pg.add_task(es, total=epochs, res=\"\", visible=epochs &gt; 1)\n  pg.advance(pg.tasks[es], start_epoch - 1)\n  if ss in pg.tasks:\n    pg.pg.reset(pg.tasks[ss], total=len(dl) * epochs, res=\"\")\n  else:\n    pg.add_task(ss, total=len(dl) * epochs, res=\"\")\n  pg.advance(pg.tasks[ss], (start_epoch - 1) * len(dl))\n\n  with PGThread(pg.pg, pg.tasks[ss]) as pts, PGThread(pg.pg, pg.tasks[es]) as pte:\n    epoch = start_epoch\n    for d in dl.viter(epochs, chunks, epoch_bias=start_epoch):\n      if d.epoch != epoch:\n        pte.completed += 1\n        epoch = d.epoch\n      pts.completed += 1\n      yield d\n</code></pre>"},{"location":"api/#nadl.fallback_argsort","title":"<code>fallback_argsort(x, axis=None)</code>","text":"<p>Fallback to numpy argsort when CPU.</p> Source code in <code>nadl/data.py</code> <pre><code>@filter_jit\ndef fallback_argsort(x: jax.Array, axis: int | None = None) -&gt; jax.Array:\n  \"\"\"Fallback to numpy argsort when CPU.\"\"\"\n  if jax.devices()[0].platform == \"cpu\":\n    return jax.pure_callback(\n      _np_sort, jax.ShapeDtypeStruct(x.shape, jnp.int32), x, axis\n    )\n  return x.argsort(axis=axis)\n</code></pre>"},{"location":"api/#nadl.filter_concat","title":"<code>filter_concat(xs, filter_spec=eqx.is_array, select_idx=-1)</code>","text":"<p>Filter concat.</p> Source code in <code>nadl/utils.py</code> <pre><code>def filter_concat[T](\n  xs: Sequence[T],\n  filter_spec: Callable[[Any], bool] = eqx.is_array,\n  select_idx: int = -1,\n) -&gt; T:\n  \"\"\"Filter concat.\"\"\"\n  t1, t2 = eqx.partition(xs, filter_spec=filter_spec)\n  t1 = jax.tree.map(lambda *x: jnp.r_[*x], *t1)\n  return eqx.combine(t1, t2[select_idx])\n</code></pre>"},{"location":"api/#nadl.filter_tree","title":"<code>filter_tree(x, cond)</code>","text":"<p>Filter tree.</p> Source code in <code>nadl/utils.py</code> <pre><code>def filter_tree(x: PyTree, cond: Callable[[jax.Array], bool]) -&gt; list[jax.Array]:\n  \"\"\"Filter tree.\"\"\"\n  return jax.tree.leaves(eqx.filter(x, cond))\n</code></pre>"},{"location":"api/#nadl.get_bias","title":"<code>get_bias(x)</code>","text":"<p>Get bias of a module.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def get_bias(x: Any) -&gt; Array:  # noqa: ANN401\n  \"\"\"Get bias of a module.\"\"\"\n  return x.bias\n</code></pre>"},{"location":"api/#nadl.get_weight","title":"<code>get_weight(x)</code>","text":"<p>Get weight of a module.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def get_weight(x: Any) -&gt; Array:  # noqa: ANN401\n  \"\"\"Get weight of a module.\"\"\"\n  return x.weight\n</code></pre>"},{"location":"api/#nadl.identity_scaler","title":"<code>identity_scaler(arr, axis=0)</code>","text":"<p>Get identity scaler.</p> Source code in <code>nadl/preprocessing.py</code> <pre><code>def identity_scaler(arr: jax.Array, axis: int = 0) -&gt; SCALER:\n  \"\"\"Get identity scaler.\"\"\"\n  del arr, axis\n\n  def scaler(x: jax.Array) -&gt; jax.Array:\n    \"\"\"Scaler.\"\"\"\n    return x\n\n  return scaler\n</code></pre>"},{"location":"api/#nadl.info_nce","title":"<code>info_nce(pos, neg, t=0.07)</code>","text":"<p>Compute info nce loss for paired pos-neg data.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def info_nce(\n  pos: Float[Array, \"B D\"], neg: Float[Array, \"B D\"], t: float = 0.07\n) -&gt; Float[Array, \"B 1\"]:\n  \"\"\"Compute info nce loss for paired pos-neg data.\"\"\"\n  pos /= jnp.linalg.norm(pos, axis=-1, keepdims=True)\n  neg /= jnp.linalg.norm(neg, axis=-1, keepdims=True)\n\n  pos_sim = jnp.einsum(\"bi,bi-&gt;b\", pos, pos) / t\n  neg_sim = jnp.einsum(\"bi,nd-&gt;bn\", pos, neg) / t\n  logits = jnp.c_[pos_sim, neg_sim]\n  return cast(\n    Float[Array, \"B 1\"],\n    softmax_cross_entropy_with_integer_labels(logits, jnp.arange(logits.shape[0])),\n  )\n</code></pre>"},{"location":"api/#nadl.init_fn","title":"<code>init_fn(fn)</code>","text":"<p>Initialize function.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def init_fn(fn: Callable[[K, tuple[int, ...]], F]) -&gt; Callable[[F, K], F]:\n  \"\"\"Initialize function.\"\"\"\n\n  def _init_fn(weight: F, key: K) -&gt; F:\n    return fn(key, jnp.shape(weight))\n\n  return _init_fn\n</code></pre>"},{"location":"api/#nadl.init_surgery","title":"<code>init_surgery(model, key, is_leaf=is_linear, init=kaiming_init, get_weight=get_weight)</code>","text":"<p>Initialize model.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def init_surgery[T, M](\n  model: T,\n  key: K,\n  is_leaf: Callable[[M], bool] = is_linear,\n  init: Callable[[F, K], F] = kaiming_init,\n  get_weight: Callable = get_weight,\n) -&gt; T:\n  \"\"\"Initialize model.\"\"\"\n\n  def _get_weights(m: Any) -&gt; list[F]:  # noqa: ANN401\n    return list(\n      map(\n        get_weight,\n        filter(is_leaf, jax.tree.leaves(m, is_leaf=is_leaf)),\n      )\n    )\n\n  weights = _get_weights(model)\n  new_weights = list(map(init, weights, jax.random.split(key, len(weights))))\n  return eqx.tree_at(_get_weights, model, new_weights)\n</code></pre>"},{"location":"api/#nadl.iou_coef","title":"<code>iou_coef(y_true, y_pred, eps=1e-08)</code>","text":"<p>Compute intersection over union.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def iou_coef(\n  y_true: Int[Array, \" A\"], y_pred: Int[Array, \" A\"], eps: float = 1e-8\n) -&gt; Scalar:\n  \"\"\"Compute intersection over union.\"\"\"\n  y_true = jnp.asarray(y_true)\n  y_pred = jnp.asarray(y_pred)\n\n  intersection = jnp.sum(y_true * y_pred)\n  union = jnp.sum(y_true) + jnp.sum(y_pred) - intersection\n\n  return intersection / (union + eps)\n</code></pre>"},{"location":"api/#nadl.is_conv","title":"<code>is_conv(x)</code>","text":"<p>Check if a module is a convolution layer.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def is_conv(x: Any) -&gt; TypeGuard[eqx.nn.Conv]:  # noqa: ANN401\n  \"\"\"Check if a module is a convolution layer.\"\"\"\n  return isinstance(x, eqx.nn.Conv)\n</code></pre>"},{"location":"api/#nadl.is_conv1d","title":"<code>is_conv1d(x)</code>","text":"<p>Check if a module is a 1D convolution layer.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def is_conv1d(x: Any) -&gt; TypeGuard[eqx.nn.Conv1d]:  # noqa: ANN401\n  \"\"\"Check if a module is a 1D convolution layer.\"\"\"\n  return isinstance(x, eqx.nn.Conv1d)\n</code></pre>"},{"location":"api/#nadl.is_conv2d","title":"<code>is_conv2d(x)</code>","text":"<p>Check if a module is a 2D convolution layer.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def is_conv2d(x: Any) -&gt; TypeGuard[eqx.nn.Conv2d]:  # noqa: ANN401\n  \"\"\"Check if a module is a 2D convolution layer.\"\"\"\n  return isinstance(x, eqx.nn.Conv2d)\n</code></pre>"},{"location":"api/#nadl.is_linear","title":"<code>is_linear(x)</code>","text":"<p>Check if a module is a linear layer.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def is_linear(x: Any) -&gt; TypeGuard[eqx.nn.Linear]:  # noqa: ANN401\n  \"\"\"Check if a module is a linear layer.\"\"\"\n  return isinstance(x, eqx.nn.Linear)\n</code></pre>"},{"location":"api/#nadl.kaiming_init","title":"<code>kaiming_init(weight, key)</code>","text":"<p>Kaiming initialization.</p> Source code in <code>nadl/surgery.py</code> <pre><code>def kaiming_init(weight: F, key: K) -&gt; F:\n  \"\"\"Kaiming initialization.\"\"\"\n  return init_fn(jax.nn.initializers.he_normal())(weight, key)\n</code></pre>"},{"location":"api/#nadl.min_max_scaler","title":"<code>min_max_scaler(arr, axis=0)</code>","text":"<p>Get min max scaler.</p> Source code in <code>nadl/preprocessing.py</code> <pre><code>def min_max_scaler(arr: jax.Array, axis: int = 0) -&gt; SCALER:\n  \"\"\"Get min max scaler.\"\"\"\n  min_, max_ = arr.min(axis=axis, keepdims=True), arr.max(axis=axis, keepdims=True)\n\n  def scaler(x: jax.Array) -&gt; jax.Array:\n    \"\"\"Scaler.\"\"\"\n    return (x - min_) / (max_ - min_)\n\n  return scaler\n</code></pre>"},{"location":"api/#nadl.normalizer","title":"<code>normalizer(arr, axis=0, norm='l2')</code>","text":"<p>Get normalizer.</p> Source code in <code>nadl/preprocessing.py</code> <pre><code>def normalizer(\n  arr: jax.Array, axis: int = 0, norm: Literal[\"l1\", \"l2\", \"max\"] = \"l2\"\n) -&gt; SCALER:\n  \"\"\"Get normalizer.\"\"\"\n  match norm:\n    case \"l2\":\n      norm_value = jnp.sqrt(jnp.sum(jnp.square(arr), axis=axis, keepdims=True))\n    case \"l1\":\n      norm_value = jnp.sum(jnp.abs(arr), axis=axis, keepdims=True)\n    case \"max\":\n      norm_value = jnp.max(jnp.abs(arr), axis=axis, keepdims=True)\n    case _:\n      raise ValueError(\"norm should be 'l1', 'l2', or 'max'\")\n\n  def scaler(x: jax.Array) -&gt; jax.Array:\n    \"\"\"Scaler.\"\"\"\n    max_val = jnp.maximum(norm_value, jnp.finfo(x.dtype).tiny)  # Avoid division by zero\n    return x / max_val\n\n  return scaler\n</code></pre>"},{"location":"api/#nadl.pformat","title":"<code>pformat(xs, short_arrays=False)</code>","text":"<p>Pretty format.</p> Source code in <code>nadl/utils.py</code> <pre><code>def pformat(xs: PyTree, short_arrays: bool = False) -&gt; str:\n  \"\"\"Pretty format.\"\"\"\n  with (console := Console()).capture() as capture:\n    nxs = eqx.tree_pformat(xs, short_arrays=short_arrays)\n    console.print(nxs, soft_wrap=True, justify=\"left\", no_wrap=True, width=40)\n  return capture.get()\n</code></pre>"},{"location":"api/#nadl.pr_auc_score","title":"<code>pr_auc_score(labels, preds)</code>","text":"<p>Compute PR.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def pr_auc_score(labels: Num[Array, \" A\"], preds: Num[Array, \" A\"]) -&gt; Scalar:\n  \"\"\"Compute PR.\"\"\"\n\n  def _callback(lbl: Num[Array, \" A\"], prd: Num[Array, \" A\"]) -&gt; Scalar:\n    precision, recall, _ = m.precision_recall_curve(lbl, prd)\n    return jnp.asarray(m.auc(recall, precision))\n\n  return filter_pure_callback(\n    _callback,\n    labels,\n    preds,\n    result_shape_dtypes=jax.ShapeDtypeStruct((), jnp.float32),\n  )\n</code></pre>"},{"location":"api/#nadl.resnet101","title":"<code>resnet101(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet101.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnet101(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet101.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 23, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnet152","title":"<code>resnet152(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet152.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnet152(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet152.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 8, 36, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnet18","title":"<code>resnet18(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet18.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnet18(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet18.\"\"\"\n  return ResNet.init(\n    BasicBlock,\n    [2, 2, 2, 2],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnet34","title":"<code>resnet34(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet34.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnet34(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet34.\"\"\"\n  return ResNet.init(\n    BasicBlock,\n    [3, 4, 6, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnet50","title":"<code>resnet50(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet50.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnet50(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet50.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 6, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnext101_32x8d","title":"<code>resnext101_32x8d(key, num_classes=1000, zero_init_residual=False, groups=32, width_per_group=8, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet101_32x8d.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnext101_32x8d(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 32,\n  width_per_group: int = 8,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet101_32x8d.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 23, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnext101_64x4d","title":"<code>resnext101_64x4d(key, num_classes=1000, zero_init_residual=False, groups=64, width_per_group=4, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet101_64x4d.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnext101_64x4d(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 64,\n  width_per_group: int = 4,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet101_64x4d.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 23, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.resnext50_32x4d","title":"<code>resnext50_32x4d(key, num_classes=1000, zero_init_residual=False, groups=32, width_per_group=4, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>ResNet50_32x4d.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def resnext50_32x4d(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 32,\n  width_per_group: int = 4,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"ResNet50_32x4d.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 6, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.rle","title":"<code>rle(x, shift=1)</code>","text":"<p>Run length encoding.</p> Source code in <code>nadl/utils.py</code> <pre><code>def rle(x: jax.Array, shift: int = 1) -&gt; str:\n  \"\"\"Run length encoding.\"\"\"\n  return \" \".join(map(str, rle_array(x, shift)))\n</code></pre>"},{"location":"api/#nadl.rle_array","title":"<code>rle_array(x, shift=1)</code>","text":"<p>Run length encoding array.</p> Source code in <code>nadl/utils.py</code> <pre><code>def rle_array(x: jax.Array, shift: int = 1) -&gt; jax.Array:\n  \"\"\"Run length encoding array.\"\"\"\n  x = x.flatten()\n  x = jnp.pad(x, (1, 1), mode=\"constant\")\n  x = jnp.argwhere(x[1:] != x[:-1]).flatten() + shift\n  return x.at[1::2].add(-x[::2])\n</code></pre>"},{"location":"api/#nadl.roc_auc_score","title":"<code>roc_auc_score(labels, preds)</code>","text":"<p>Compute ROC.</p> Source code in <code>nadl/metrics.py</code> <pre><code>def roc_auc_score(labels: Num[Array, \" A\"], preds: Num[Array, \" A\"]) -&gt; Scalar:\n  \"\"\"Compute ROC.\"\"\"\n  return jax.pure_callback(\n    m.roc_auc_score, jax.ShapeDtypeStruct((), jnp.float32), labels, preds\n  )\n</code></pre>"},{"location":"api/#nadl.select_scaler","title":"<code>select_scaler(method='minmax', axis=0)</code>","text":"<p>Get scaler function.</p> Source code in <code>nadl/preprocessing.py</code> <pre><code>def select_scaler(\n  method: Literal[\"id\", \"minmax\", \"std\", \"l2_norm\", \"l1_norm\", \"max_norm\"] = \"minmax\",\n  axis: int = 0,\n) -&gt; Callable[[jax.Array], SCALER]:\n  \"\"\"Get scaler function.\"\"\"\n  match method:\n    case \"id\":\n      return partial(identity_scaler, axis=axis)\n    case \"minmax\":\n      return partial(min_max_scaler, axis=axis)\n    case \"std\":\n      return partial(standard_scaler, axis=axis)\n    case \"l2_norm\":\n      return partial(normalizer, norm=\"l2\", axis=axis)\n    case \"l1_norm\":\n      return partial(normalizer, norm=\"l1\", axis=axis)\n    case \"max_norm\":\n      return partial(normalizer, norm=\"max\", axis=axis)\n    case _:\n      raise ValueError(f\"Unknown scaler method {method}\")\n</code></pre>"},{"location":"api/#nadl.standard_scaler","title":"<code>standard_scaler(arr, axis=0)</code>","text":"<p>Get standard scaler.</p> Source code in <code>nadl/preprocessing.py</code> <pre><code>def standard_scaler(arr: jax.Array, axis: int = 0) -&gt; SCALER:\n  \"\"\"Get standard scaler.\"\"\"\n  mean, std = arr.mean(axis=axis, keepdims=True), arr.std(axis=axis, keepdims=True)\n\n  def scaler(x: jax.Array) -&gt; jax.Array:\n    \"\"\"Scaler.\"\"\"\n    return (x - mean) / std\n\n  return scaler\n</code></pre>"},{"location":"api/#nadl.state_fn","title":"<code>state_fn(rpath, console=None, keeps=5, clean=False, item_names=None, item_handlers=None, best_fn=None)</code>","text":"<p>Get states manager.</p> Source code in <code>nadl/states.py</code> <pre><code>def state_fn(\n  rpath: Path,\n  console: Console | None = None,\n  keeps: int = 5,\n  clean: bool = False,\n  item_names: tuple[str, ...] | None = None,\n  item_handlers: dict | None = None,\n  best_fn: Callable[[PyTree], float] | None = None,\n) -&gt; tuple[ocp.CheckpointManager, T_savefn]:\n  \"\"\"Get states manager.\"\"\"\n  match (item_names, item_handlers):\n    case (None, None):\n      item_names = (\"state\", \"extra_metadata\")\n      item_handlers = {\n        \"state\": ocp.PyTreeCheckpointHandler(),\n        \"extra_metadata\": ocp.PyTreeCheckpointHandler(),\n      }\n    case _ if item_names and item_handlers:\n      for i in item_names:\n        assert i in item_handlers, f\"Item {i} not in item_handlers.\"\n    case _:\n      raise ValueError(\"item_names and item_handlers should be both None or not None.\")\n\n  if console:\n    console.log(f\"Checkpoint path at {rpath}\")\n  if rpath.exists() and clean:\n    if console:\n      console.log(\"Cleaning up checkpoint...\")\n    shutil.rmtree(rpath)\n\n  mngr = ocp.CheckpointManager(\n    rpath,\n    options=ocp.CheckpointManagerOptions(\n      max_to_keep=keeps, save_interval_steps=1, best_fn=best_fn\n    ),\n    item_names=item_names,\n    item_handlers=item_handlers,\n  )\n\n  def save(\n    step: int, state: BaseTrainState, metadata: PyTree = None, metrics: PyTree = None\n  ) -&gt; None:\n    \"\"\"Save state.\"\"\"\n    mngr.save(\n      step,\n      args=ocp.args.Composite(\n        state=ocp.args.PyTreeSave(state),  # type: ignore\n        extra_metadata=ocp.args.PyTreeSave(metadata or {}),  # type: ignore\n      ),\n      metrics=metrics or {},\n    )\n\n  return mngr, save\n</code></pre>"},{"location":"api/#nadl.wide_resnet101_2","title":"<code>wide_resnet101_2(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>Wide ResNet101_2.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def wide_resnet101_2(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"Wide ResNet101_2.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 23, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"},{"location":"api/#nadl.wide_resnet50_2","title":"<code>wide_resnet50_2(key, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, activation=jax.nn.relu)</code>","text":"<p>Wide ResNet50_2.</p> Source code in <code>nadl/resnet.py</code> <pre><code>def wide_resnet50_2(\n  key: PRNGKeyArray,\n  num_classes: int = 1000,\n  zero_init_residual: bool = False,\n  groups: int = 1,\n  width_per_group: int = 64,\n  replace_stride_with_dilation: list[bool] | None = None,\n  norm_layer: type[Norm] | None = None,\n  activation: Callable[[Array], Array] = jax.nn.relu,\n) -&gt; ResNet:\n  \"\"\"Wide ResNet50_2.\"\"\"\n  return ResNet.init(\n    Bottleneck,\n    [3, 4, 6, 3],\n    num_classes=num_classes,\n    zero_init_residual=zero_init_residual,\n    groups=groups,\n    width_per_group=width_per_group,\n    replace_stride_with_dilation=replace_stride_with_dilation,\n    norm_layer=norm_layer,\n    activation=activation,\n    key=key,\n  )\n</code></pre>"}]}